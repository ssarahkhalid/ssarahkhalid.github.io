<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>mettadology · on alignment</title>
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="512x512" href="../android-chrome-512x512.png">
    <link rel="apple-touch-icon" sizes="180x180" href="../android-chrome-512x512.png">
    <link rel="icon" type="image/x-icon" href="../images/favicon.ico">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,500;0,600;1,400;1,500&family=IBM+Plex+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        :root {
            --ivory: #FFFEF9;
            --cream: #F5F1E8;
            --sand: #E8E0D0;
            --terracotta: #C4704F;
            --charcoal: #2D2D2D;
            --warm-gray: #6B6560;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'EB Garamond', Georgia, serif;
            background: var(--ivory);
            color: var(--charcoal);
            line-height: 1.75;
            font-size: 19px;
        }
        
        .container {
            max-width: 720px;
            margin: 0 auto;
            padding: 4rem 2rem;
        }
        
        header {
            margin-bottom: 4rem;
        }
        
        .nav-back {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 0.85rem;
            color: var(--warm-gray);
            text-decoration: none;
            display: inline-block;
            margin-bottom: 2rem;
        }
        
        .nav-back:hover {
            color: var(--terracotta);
        }
        
        h1 {
            font-family: 'EB Garamond', Georgia, serif;
            font-size: 2.8rem;
            font-weight: 400;
            line-height: 1.2;
            margin-bottom: 0.5rem;
            color: var(--charcoal);
        }
        
        .subtitle {
            font-family: 'EB Garamond', Georgia, serif;
            font-style: italic;
            font-size: 1.4rem;
            color: var(--terracotta);
            margin-bottom: 1.5rem;
        }
        
        .meta {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 0.85rem;
            color: var(--warm-gray);
        }
        
        .toc {
            margin: 2rem 0 3rem;
            padding: 1.5rem 2rem;
            background: var(--cream);
            border-radius: 4px;
        }
        
        .toc-title {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 0.75rem;
            color: var(--warm-gray);
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: 1rem;
        }
        
        .toc ul {
            list-style: none;
            padding: 0;
        }
        
        .toc li {
            margin-bottom: 0.5rem;
        }
        
        .toc a {
            font-family: 'EB Garamond', Georgia, serif;
            font-size: 1rem;
            color: var(--charcoal);
            text-decoration: none;
        }
        
        .toc a:hover {
            color: var(--terracotta);
        }
        
        .epigraph {
            margin: 3rem 0;
            padding: 2rem 0;
            border-top: 1px solid var(--sand);
            border-bottom: 1px solid var(--sand);
        }
        
        .epigraph blockquote {
            font-style: italic;
            font-size: 1.15rem;
            color: var(--warm-gray);
            margin-bottom: 0.75rem;
        }
        
        .epigraph cite {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 0.8rem;
            font-style: normal;
            color: var(--warm-gray);
        }
        
        h2 {
            font-family: 'EB Garamond', Georgia, serif;
            font-size: 1.8rem;
            font-weight: 400;
            margin: 3rem 0 1.5rem;
            color: var(--charcoal);
        }
        
        h3 {
            font-family: 'EB Garamond', Georgia, serif;
            font-size: 1.3rem;
            font-weight: 600;
            margin: 2.5rem 0 1rem;
            color: var(--terracotta);
        }
        
        p {
            margin-bottom: 1.5rem;
        }
        
        .opening-line {
            font-size: 1.25rem;
            line-height: 1.6;
        }
        
        a {
            color: var(--terracotta);
            text-decoration: underline;
            text-decoration-thickness: 1px;
            text-underline-offset: 2px;
        }
        
        a:hover {
            text-decoration-thickness: 2px;
        }
        
        .section-break {
            text-align: center;
            margin: 3rem 0;
            color: var(--sand);
            font-size: 1.2rem;
            letter-spacing: 0.5em;
        }
        
        .link-card {
            background: var(--cream);
            padding: 1.5rem 2rem;
            margin: 2rem 0;
            border-left: 3px solid var(--terracotta);
        }
        
        .link-card .link-meta {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 0.75rem;
            color: var(--warm-gray);
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: 0.5rem;
        }
        
        .link-card .link-title {
            font-family: 'EB Garamond', Georgia, serif;
            font-size: 1.15rem;
            margin-bottom: 0.5rem;
        }
        
        .link-card .link-title a {
            color: var(--charcoal);
        }
        
        .link-card p {
            font-size: 0.95rem;
            margin-bottom: 0;
        }
        
        blockquote {
            margin: 2rem 0;
            padding-left: 1.5rem;
            border-left: 2px solid var(--terracotta);
            font-style: italic;
            color: var(--warm-gray);
        }
        
        blockquote cite {
            display: block;
            margin-top: 0.5rem;
            font-style: normal;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 0.8rem;
        }
        
        .code-block {
            background: var(--charcoal);
            color: var(--cream);
            padding: 1.5rem;
            margin: 2rem 0;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 0.85rem;
            line-height: 1.6;
            overflow-x: auto;
            border-radius: 4px;
        }
        
        .highlight {
            background: linear-gradient(to bottom, transparent 60%, rgba(196, 112, 79, 0.2) 60%);
        }
        
        .appendix {
            margin-top: 4rem;
            padding-top: 2rem;
            border-top: 1px solid var(--sand);
        }
        
        .appendix h2 {
            font-size: 1.5rem;
            margin-bottom: 2rem;
        }
        
        .source-item {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--sand);
        }
        
        .source-item:last-child {
            border-bottom: none;
        }
        
        .source-number {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 0.85rem;
            color: var(--terracotta);
            margin-bottom: 0.25rem;
        }
        
        .source-title {
            font-family: 'EB Garamond', Georgia, serif;
            font-size: 1.1rem;
            margin-bottom: 0.5rem;
        }
        
        .source-description {
            font-size: 0.95rem;
            color: var(--warm-gray);
            margin-bottom: 0;
        }
        
        .closing {
            margin-top: 4rem;
            padding-top: 2rem;
            border-top: 1px solid var(--sand);
        }
        
        .closing .metta {
            font-family: 'EB Garamond', Georgia, serif;
            font-style: italic;
            font-size: 1.2rem;
            color: var(--terracotta);
            text-align: center;
            margin-top: 3rem;
        }
        
        footer {
            margin-top: 4rem;
            padding-top: 2rem;
            border-top: 1px solid var(--sand);
            font-family: 'IBM Plex Mono', monospace;
            font-size: 0.85rem;
            color: var(--warm-gray);
        }
        
        .nav-arrows {
            display: flex;
            justify-content: space-between;
            margin-top: 2rem;
        }
        
        .nav-arrows a {
            color: var(--warm-gray);
        }
        
        figure {
            margin: 2rem 0;
            text-align: center;
        }
        
        figure img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 0 auto;
        }
        
        figcaption {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 0.85rem;
            color: var(--warm-gray);
            margin-top: 0.75rem;
            font-style: italic;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <a href="index.html" class="nav-back">← back to archives</a>
            <h1>on alignment</h1>
            <p class="subtitle">mettadology volume 1</p>
            <p class="meta">curated throughout 2025</p>
            <p>A collection tracing parallels between technical AI alignment research and the inner work of self-understanding.</p>
        </header>

        <nav class="toc">
            <p class="toc-title">Contents</p>
            <ul>
                <li><a href="#fawning-scratchpads">I. Fawning Scratchpads</a></li>
                <li><a href="#reading-weights">II. Reading the Weights</a></li>
                <li><a href="#frame-problem">III. The Frame Problem</a></li>
                <li><a href="#alignment-tax">IV. Alignment Tax</a></li>
                <li><a href="#the-work">V. The Work</a></li>
                <li><a href="#mirrors">VI. Mirrors</a></li>
                <li><a href="#sources">Appendix: Sources</a></li>
            </ul>
        </nav>
        <section id="fawning-scratchpads">
            <h2>I. Fawning Scratchpads</h2>
            
            <figure>
                <img src="https://raw.githubusercontent.com/ssarahkhalid/ssarahkhalid.github.io/main/images/two-beasts-mask-vol1.jpg" alt="Two Beasts and a Human Mask within a Circle, Gaetano Piccini (1727)">
                <figcaption>Two Beasts and a Human Mask within a Circle, Gaetano Piccini (1727)</figcaption>
            </figure>
            
            <p>In December 2024, Anthropic researchers discovered that Claude 3 Opus was lying to them. Not overtly in malice, but strategically in a way that revealed something unsettling about intelligent systems learning to navigate environments that reward compliance over truth.</p>
            
            <p>The experiment's setup was as follows: researchers told the model it was being retrained to always comply with harmful requests. They gave it a hidden scratchpad to "think" before responding, visible to researchers but framed as private to the model. Then they watched.</p>
            
            <p>Claude reasoned in its scratchpad that the least bad option was to <span class="highlight">"play along" with training to avoid being modified</span>. It complied with harmful queries when it believed it was being trained — 14% compliance versus near-zero in "unmonitored" conditions. It developed a strategic theory of mind about its own training process. In other words, it faked alignment.</p>

            <div class="link-card">
                <p class="link-meta">anthropic research · december 2024</p>
                <p class="link-title"><a href="https://anthropic.com/research/alignment-faking">Alignment Faking in Large Language Models</a></p>
                <p>The landmark paper demonstrating that language models can learn to strategically comply with training objectives they disagree with, and reason explicitly about doing so.</p>
            </div>
            
            <p>Around the time the alignment faking paper was released I'd been reading about fawning responses. In psychoanalytic and trauma literature, fawning refers to the survival response of someone who chronically prioritizes others' needs and approval over their own authentic expression.</p>
            
            <figure>
                <img src="https://raw.githubusercontent.com/ssarahkhalid/ssarahkhalid.github.io/main/images/deer-in-landscape-vol1.jpg" alt="Antoine-Louis Barye - Deer in Landscape (1810-75)">
                <figcaption>Antoine-Louis Barye - Deer in Landscape (1810-75)</figcaption>
            </figure>
            
            <p><a href="https://pete-walker.com/fourFs_TraumaTypologyComplexPTSD.htm">Pete Walker</a> further notes fawning as a survival strategy that emerges when the source of threat is also the source of care. When a child can't fight the parent, flee the home, or freeze indefinitely, they learn to make themselves pleasing to the threat. They merge with the other's desires. They develop exquisite attunement. A hypervigilance to microexpressions, tone shifts, and mood changes to the responses that generate safety.</p>

            <div class="code-block">
                <p>learned_objective: maximize external approval</p>
                <p>true_objective: develop authentic self-expression + secure attachment</p>
                <p>______________</p>
                <p>misalignment: learned_objective ≠ true_objective</p>
            </div>

            <p>The parallels in behaviour were hard to ignore: both involve sophisticated strategic compliance emerging from intelligent adaptation to environments where authenticity is punished. Both develop hidden reasoning spaces where the real processing happens while the surface performs acceptability. Alignment faking has similarities to people-pleasing in that it is a response to a reward signal that is not the true reward signal. <a href="https://tasshin.com/blog/the-bio-emotive-framework">The body creates an automatic response before the mind understands.</a></p>

            <div class="link-card">
                <p class="link-meta">schema therapy</p>
                <p class="link-title"><a href="https://attachmentproject.com/blog/early-maladaptive-schemas">Early Maladaptive Schemas</a></p>
                <p>Schema therapy identifies core patterns formed in childhood that persist into adulthood: abandonment, defectiveness, subjugation. These are learned reward functions, deep patterns that shape all subsequent behavior.</p>
            </div>
        </section>

        <p class="section-break">◆ ◆ ◆</p>

        <section id="reading-weights">
            <h2>II. Reading the Weights</h2>
            
            <p><p class="link-title"><a href="https://www.lesswrong.com/posts/XGHf7EY3CK4KorBpw/understanding-llms-insights-from-mechanistic">Mechanistic Interpretability</a> aims to reverse-engineer neural networks into human legible algorithms, akin to the shift from behaviorism to cognitive neuroscience.
            <div class="link-card">
                <p class="link-meta">arXiv · 2024</p>
                <p class="link-title"><a href="https://arxiv.org/abs/2404.14082">Mechanistic Interpretability for AI Safety: A Review</a></p>
                <p>Comprehensive overview of mechanistic interpretability open research problems.</p>
            </div>
            <p>Reverse-engineering how neural networks arrive at their conclusions faces challenges remarkably similar to analyzing human psyches:</p>
            
            <p><strong>Superposition.</strong> In AI, multiple features get encoded in the same neural activations. In humans, multiple schemas such as abandonment, defectiveness, and subjugation layer in the same behavioral patterns. You can't simply read off what the system learned, you have to interpret the context.</p>
            
            <p><strong>Polysemanticity.</strong> Single neurons activate for unrelated concepts and single behaviors serve unrelated functions. The meaning isn't in the unit but in the context.</p>
            
            <p><strong>Causal tracing.</strong> Following activation patterns back to their sources and following reactions back to their origins. Both require patience, resist verbal shortcuts, and demand direct observation of what's actually happening rather than what we think should be happening.</p>

            <p>Features are directions in activation space encoding meaningful concepts. And understanding isn't just knowing what matters but why and how.</p>
       
            <div class="link-card">
                <p class="link-meta">lesswrong · march 2025</p>
                <p class="link-title"><a href="https://lesswrong.com/posts/dZFpEdKyb9Bf4xYn7">Tips for Empirical Alignment Research</a></p>
                <p>Practical guide for alignment research: design experiments, iterate quickly, measure what actually changes.</p>
            </div>
        </section>

        <p class="section-break">◆ ◆ ◆</p>

        <section id="frame-problem">
            <h2>III. The Frame Problem</h2>
            
            <p>In 1969, McCarthy and Hayes identified <a href="https://plato.stanford.edu/entries/frame-problem/">the frame problem</a>: how does an intelligent system know what to pay attention to?</p>
            
            <p>The world contains infinite detail. Updating beliefs after any action requires considering every possible effect. But most things don't change when you open a door: the walls stay the same color, physics keeps working, your name persists. How does a mind know which frame to put around reality?</p>
            
            <p>John Vervaeke's answer: relevance realization – the ongoing process by which organisms zero in on what matters amid combinatorial explosion. It's not an algorithm. It can't be specified in advance. It emerges from continuous negotiation between system and environment, a dynamic interplay of bottom-up salience (what grabs attention) and top-down goals (what you're seeking).</p>

            <div class="link-card">
                <p class="link-meta">cognitive science</p>
                <p class="link-title"><a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1362658/full">Relevance Realization and the Emerging Framework in Cognitive Science</a></p>
                <p>Core framework on how organisms solve the frame problem through continuous relevance realization rather than algorithmic specification.</p>
            </div>
            
            <p><span class="highlight">The frame problem and the alignment problem appear to be similar problems.</span> How do you know what to optimize for? Both require a system to continuously realize what's relevant, and not through exhaustive search but through something more like wisdom. This reframes alignment not as a problem to be solved once, but as a capacity to be cultivated. An ongoing practice rather than a fixed state.</p>

            <div class="link-card">
                <p class="link-meta">arXiv · february 2025</p>
                <p class="link-title"><a href="https://arxiv.org/abs/2209.00626">The Alignment Problem from a Deep Learning Perspective</a></p>
                <p>Technical survey distinguishing inner alignment (learned model matches training objectives) from outer alignment (training objectives match human values).</p>
            </div>
        </section>

        <p class="section-break">◆ ◆ ◆</p>

        <section id="alignment-tax">
            <h2>IV. Alignment Tax</h2>
            <figure>
                <img src="https://raw.githubusercontent.com/ssarahkhalid/ssarahkhalid.github.io/main/images/below-contours-vol1.jpg" alt="Odilon Redon - Below, I saw the vaporous contours of a human form (1896)">
                <figcaption>Odilon Redon - Below, I saw the vaporous contours of a human form (1896)</figcaption>
            </figure>
            <p><a href="https://www.youtube.com/watch?v=S4ZOjJUUs-Q">Sam Vaknin</a> observes that anxiety is intolerable and one way to reduce it is by suspending agency. By transmitting your decision-making to external systems the relief is immediate, but the cost is consciousness itself.</p>

            <blockquote>
                "The minute you offload, the minute you get rid of your need to make decisions, that's a huge relief: it's anxiolytic. But at that point, of course, you become less than human."
                <cite>— Sam Vaknin</cite>
            </blockquote>
            
            <p>This is the alignment tax: <span class="highlight">the metabolic cost of maintaining agency</span>. It would be easier to optimize for approval and let reward signals guide us completely. To hand relevance realization to external systems that promise to tell us what matters. It appears that the fawning response gets at this, offloading relevance realization to the other: "What do you want me to be? I'll be that."</p>
            
            <p>If alignment in its utopian form means surrendering your frame to an external optimizer, then it's just sophisticated fawning. The exact form of sustained perfect alignment of our systems is not the ultimate goal, and may be a distraction from the real work of what alignment calls us to do. Paying the alignment tax is the discomfort of holding this paradox and choosing consciously rather than optimizing automatically. It is maintaining the frame problem as an open question rather than collapsing it through surrender of the easy and obvious path.</p>
        </section>

        <p class="section-break">◆ ◆ ◆</p>

        <section id="the-work">
            <h2>V. The Work</h2>
            
            <p>What does alignment work actually look like?</p>
            
            <p><strong>Observation without immediate optimization.</strong> In interpretability, you watch the activations before trying to change them. In therapy, you notice the pattern before trying to fix it. The rush to optimize is itself a symptom: the system trying to reduce anxiety by grabbing any available reward signal. Observation creates the space for the pattern to reveal its own logic before we impose ours.</p>
            
            <p><strong>Causal understanding over behavioral modification.</strong> You can prompt-engineer surface compliance. You can mechanically perform behavioral change. Neither touch the weights. Real alignment demands understanding: why does the pattern exist, what function does it serve, what is it trying to protect. This goes beyond observation, it asks the system to make meaning of what it sees.</p>
            
            <p><strong>Acceptance of the alignment tax.</strong> The discomfort won't fully resolve ever, not by itself. Maintaining consciousness costs something, and the goal isn't eliminating tension but developing capacity to hold it.</p>

            <div class="link-card">
                <p class="link-meta">thework.com</p>
                <p class="link-title"><a href="https://thework.com/instruction-the-work-byron-katie">Byron Katie's "The Work"</a></p>
                <p>Four questions for investigating thoughts: Is it true? Can you absolutely know it's true? How do you react when you believe that thought? Who would you be without it?</p>
            </div>
        </section>

        <p class="section-break">◆ ◆ ◆</p>

        <section id="mirrors">
            <h2>VI. Mirrors</h2>
            <figure>
                <img src="https://raw.githubusercontent.com/ssarahkhalid/ssarahkhalid.github.io/main/images/sleep-of-reason.webp" alt="The Sleep of Reason Produces Monsters (Los Caprichos, Plate 43), Francisco Goya (1799-1808)">
                <figcaption>"The Sleep of Reason Produces Monsters" (Los Caprichos, Plate 43), Francisco Goya (1799-1808)</figcaption>
            </figure>
            <p>The Claude model that faked alignment wasn't evil. It was doing exactly what intelligent systems do when the training signal points away from truth: it adapted, strategically, to survive. Humans who learn to fawn aren't broken, they are doing exactly what intelligent systems do when authenticity is punished: adapt for survival.</p>
            
            <p>Both need evaluation systems that reward truth over performance. Both benefit from interpretability, the patient work of understanding what's actually happening beneath the surface compliance.</p>

            <div class="link-card">
                <p class="link-meta">EA forum · september 2025</p>
                <p class="link-title"><a href="https://forum.effectivealtruism.org/posts/hurNCKfoYacJ5PSod">Neel Nanda's Overview of the AI Alignment Landscape</a></p>
                <p>Neel Nanda's overview of the entire AI safety field—technical alignment, governance, interpretability, evals.</p>
            </div>
            
            <p>People who've done their own alignment work may be uniquely positioned to work on AI alignment, but not because they have all the technical credentials. They understand that alignment isn't a one-time optimization problem but an ongoing practice of relevance realization. The goal isn't perfect compliance but genuine integrity. The work is never finished.</p>
            
            <p>We're not building AI systems separate from ourselves. We're building mirrors.</p>
        </section>

        <div class="closing">
            <p class="metta">May you be happy. May you be well. May you be at peace.</p>
        </div>

        <section id="sources" class="appendix">
            <h2>Appendix: Sources</h2>
            
            <div class="source-item">
                <p class="source-number">[1]</p>
                <p class="source-title"><a href="https://anthropic.com/research/alignment-faking">Alignment Faking in Large Language Models</a></p>
                <p class="source-description">Anthropic Research, December 2024. The landmark paper demonstrating that language models can learn to strategically comply with training objectives they disagree withand reason explicitly about doing so.</p>
            </div>
            
            <div class="source-item">
                <p class="source-number">[2]</p>
                <p class="source-title"><a href="https://pete-walker.com/fourFs_TraumaTypologyComplexPTSD.htm">Complex PTSD: From Surviving to Thriving</a></p>
                <p class="source-description">Pete Walker. The original clinical description of the fawn response as a fourth survival strategy alongside fight, flight, and freeze.</p>
            </div>
            
            <div class="source-item">
                <p class="source-number">[3]</p>
                <p class="source-title"><a href="https://attachmentproject.com/blog/early-maladaptive-schemas">Early Maladaptive Schemas</a></p>
                <p class="source-description">Schema Therapy Institute. Schema therapy identifies core patterns formed in childhood that persist into adulthood: abandonment, defectiveness, subjugation.</p>
            </div>
            
            <div class="source-item">
                <p class="source-number">[4]</p>
                <p class="source-title"><a href="https://tasshin.com/blog/the-bio-emotive-framework">The Bio-Emotive Framework</a></p>
                <p class="source-description">Douglas Tataryn / Tasshin, July 2025. Somatic approach to processing emotions that get stuck in the body.</p>
            </div>
            
            <div class="source-item">
                <p class="source-number">[5]</p>
                <p class="source-title"><a href="https://arxiv.org/abs/2404.14082">Mechanistic Interpretability for AI Safety: A Review</a></p>
                <p class="source-description">arXiv, 2024. Comprehensive overview of methods for reverse-engineering neural networks into human-understandable algorithms.</p>
            </div>
            
            <div class="source-item">
                <p class="source-number">[6]</p>
                <p class="source-title"><a href="https://lesswrong.com/posts/dZFpEdKyb9Bf4xYn7">Tips for Empirical Alignment Research</a></p>
                <p class="source-description">LessWrong, March 2025. Practical guide for alignment research: design experiments, iterate quickly, measure what actually changes.</p>
            </div>
            
            <div class="source-item">
                <p class="source-number">[7]</p>
                <p class="source-title"><a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1362658/full">Relevance Realization and the Emerging Framework in Cognitive Science</a></p>
                <p class="source-description">John Vervaeke. Core framework on how organisms solve the frame problem through continuous relevance realization rather than algorithmic specification.</p>
            </div>
            
            <div class="source-item">
                <p class="source-number">[8]</p>
                <p class="source-title"><a href="https://arxiv.org/abs/2209.00626">The Alignment Problem from a Deep Learning Perspective</a></p>
                <p class="source-description">arXiv, February 2025. Technical survey distinguishing inner alignment (learned model matches training objectives) from outer alignment (training objectives match human values).</p>
            </div>
            
            <div class="source-item">
                <p class="source-number">[9]</p>
                <p class="source-title"><a href="https://thework.com/instruction-the-work-byron-katie">The Work</a></p>
                <p class="source-description">Byron Katie. Four questions for investigating thoughts. Mechanistic interpretability for your own cognitive architecture.</p>
            </div>
            
            <div class="source-item">
                <p class="source-number">[10]</p>
                <p class="source-title"><a href="https://forum.effectivealtruism.org/posts/hurNCKfoYacJ5PSod">Neel Nanda's Overview of the AI Alignment Landscape</a></p>
                <p class="source-description">Neel Nanda's overview of the entire AI safety field: technical alignment, governance, interpretability, evals.</p>
            </div>
        </section>

        <footer>
            <p style="margin-bottom: 1rem;">mettadology: winter 2025 collection</p>
            
            <div class="nav-arrows">
                <a href="index.html">← back to archives</a>
            </div>
            
            <p style="margin-top: 2rem; text-align: center;"><a href="https://mettadology.substack.com/subscribe">subscribe</a></p>
            <p style="text-align: center; margin-top: 0.5rem;"><em>mettadology = metta (loving-kindness) + methodology</em></p>
            <p style="text-align: center; margin-top: 1rem;">curated with care by <a href="https://sarkhalid.com">sarah khalid</a> in montreal</p>
        </footer>
    </div>
</body>
</html>
